{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28774,"status":"ok","timestamp":1715609385696,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"VGfORq6ZN9cQ","outputId":"bc63f59d-53c5-4a74-ae0c-dd38ae31a663"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gymnasium in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (3.0.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (4.11.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (0.0.4)\n"]}],"source":["!pip install gymnasium"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1153,"status":"ok","timestamp":1715609386846,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"eQnGRUcqOK3F"},"outputs":[],"source":["import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from matplotlib.animation import FuncAnimation"]},{"cell_type":"markdown","metadata":{"id":"5PtJFZsxObHQ"},"source":["Frozen Lake es un entorno simple compuesto por casillas, donde el agente debe moverse desde una casilla inicial hasta una meta.\n","\n","Las casillas pueden ser un lago congelado seguro ‚úÖ o un agujero ‚ùå que te deja atrapado por siempre.\n","\n","El agente, tiene 4 acciones posibles: ir ‚óÄÔ∏èIZQUIERDA, üîΩABAJO, ‚ñ∂Ô∏èDERECHA o üîºARRIBA.\n","\n","El agente debe aprender a evitar los agujeros para poder alcanzar la meta en el menor n√∫mero de acciones posible.\n","\n","Por defecto, el entorno siempre tiene la misma configuraci√≥n. En el c√≥digo del entorno, cada casilla est√° representada por una letra de la siguiente manera:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6Q4YK5CTPGZi"},"outputs":[],"source":["# S F F F       (S: inicio, seguro)\n","# F H F H       (F: superficie congelada, seguro)\n","# F F F H       (H: agujero, atrapado por siempre)\n","# H F F G       (G: meta, a salvo)"]},{"cell_type":"markdown","metadata":{"id":"dRlV0hD0O1Kr"},"source":["De hecho, es muy f√°cil encontrar varias soluciones correctas: DERECHA ‚Üí DERECHA ‚Üí ABAJO ‚Üí ABAJO ‚Üí ABAJO ‚Üí DERECHA es una obvia.\n","\n","Pero podr√≠amos hacer una secuencia de acciones que rodee un agujero 10 veces antes de llegar a la meta. Esta secuencia es v√°lida, pero no cumple con nuestro requisito final: el agente necesita alcanzar la meta en el menor n√∫mero de acciones posible.\n","\n","En este ejemplo, el n√∫mero m√≠nimo de acciones para completar el juego es 6. Necesitamos recordar este hecho para comprobar si nuestro agente realmente domina Frozen Lake o no."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1715609401310,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"5loEIfuTOOAc","outputId":"68fb8e6f-627c-46fa-8027-7b94bb6f7bac"},"outputs":[{"ename":"DependencyNotInstalled","evalue":"pygame is not installed, run `pip install gymnasium[toy-text]`","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:342\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m environment \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m environment\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 3\u001b[0m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:65\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:362\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[0;32m    358\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         )\n\u001b[1;32m--> 362\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m     _check_render_return(env\u001b[38;5;241m.\u001b[39mrender_mode, result)\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:344\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install gymnasium[toy-text]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n","\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gymnasium[toy-text]`"]}],"source":["environment = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","environment.reset()\n","environment.render()"]},{"cell_type":"markdown","metadata":{"id":"jDHzy5q3REkK"},"source":["En Frozen Lake, hay 16 casillas, lo que significa que nuestro agente puede encontrarse en 16 posiciones diferentes o estados.\n","\n","Para cada estado, hay 4 acciones posibles: ir ‚óÄÔ∏èIZQUIERDA, üîΩABAJO, ‚ñ∂Ô∏èDERECHA, y üîºARRIBA.\n","\n","El agente debe aprender qu√© acci√≥n elegir en cada estado. Para saber qu√© acci√≥n es la mejor en un estado dado, vamos a calcular el Valor-Q de cada acci√≥n (Recordar que este valor es funci√≥n del estado y la acci√≥n).\n","\n","Tenemos 16 estados y 4 acciones, por lo que queremos calcular 16 x 4 = 64 valores Q."]},{"cell_type":"markdown","metadata":{"id":"ODx4VoV2RW-a"},"source":["Una forma pr√°ctica de representarlo es mediante una tabla, conocida como tabla Q, donde las filas enumeran cada estado s y las columnas enumeran cada acci√≥n a.\n","\n","En esta tabla Q, cada celda contiene el valor $Q(s, a)$, que son las recompensas esperadas de tomar la acci√≥n $a$ en el estado $s$\n","\n","Cuando nuestro agente se encuentra en un estado particular $s$, solo tiene que consultar esta tabla para ver qu√© acci√≥n tiene el valor m√°s alto."]},{"cell_type":"markdown","metadata":{"id":"I8e82ltMScc_"},"source":["``\n","S     ‚óÄÔ∏èLEFT    üîΩDOWN    ‚ñ∂Ô∏èRIGHT   üîºUP \\\n","0     Q(0,‚óÄÔ∏è)   Q(0,üîΩ)   Q(0,‚ñ∂Ô∏è)   Q(0,üîº) \\\n","1     Q(1,‚óÄÔ∏è)   Q(1,üîΩ)   Q(1,‚ñ∂Ô∏è)   Q(1,üîº) \\\n","2     Q(2,‚óÄÔ∏è)   Q(2,üîΩ)   Q(2,‚ñ∂Ô∏è)   Q(2,üîº) \\\n",". \\\n","14    Q(14,‚óÄÔ∏è)  Q(14,üîΩ)  Q(14,‚ñ∂Ô∏è)  Q(14,üîº) \\\n","G     Q(15,‚óÄÔ∏è)  Q(15,üîΩ)  Q(15,‚ñ∂Ô∏è)  Q(15,üîº)\n","``\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zFrrdi1dRVNH"},"outputs":[{"name":"stdout","output_type":"stream","text":["Q-table =\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["# Creamos la tabla y la inicializamos con ceros\n","# dimensi√≥n (filas x columnas) = (s x a) = 16 x 4\n","\n","nb_states = environment.observation_space.n  # = 16\n","nb_actions = environment.action_space.n      # = 4\n","qtable = np.zeros((nb_states, nb_actions))\n","\n","print('Q-table =')\n","print(qtable)"]},{"cell_type":"markdown","metadata":{"id":"UdTqKkgST7e-"},"source":["``\n","‚óÄÔ∏è LEFT = 0 \\\n","üîΩ DOWN = 1 \\\n","‚ñ∂Ô∏è RIGHT = 2 \\\n","üîº UP = 3\n","``"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"71tjf_CET5k7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(0, {'prob': 1})\n","1\n","recompensa: 0.0\n","Done: False\n"]},{"ename":"DependencyNotInstalled","evalue":"pygame is not installed, run `pip install gymnasium[toy-text]`","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:342\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecompensa:\u001b[39m\u001b[38;5;124m'\u001b[39m, reward)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone:\u001b[39m\u001b[38;5;124m'\u001b[39m, done)\n\u001b[1;32m---> 13\u001b[0m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:344\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install gymnasium[toy-text]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n","\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gymnasium[toy-text]`"]}],"source":["# Para tomar una acci√≥n al azar\n","print(environment.reset())\n","\n","accion = environment.action_space.sample()\n","new_state, reward, done, info, p = environment.step(accion) # Retorna done=True si el agente cae a un agujero o llega a la meta\n","\n","accion2 = environment.action_space.sample()\n","new_state, reward, done, info, p = environment.step(accion2) # Retorna done=True si el agente cae a un agujero o llega a la meta\n","\n","print(accion)\n","print(f'recompensa:', reward)\n","print(f'Done:', done)\n","environment.render()"]},{"cell_type":"markdown","metadata":{"id":"U7r0PUxZWTzu"},"source":["La recompensa es 0 y solo un estado puede darnos una recompensa positiva en todo el juego. Si queremos ver una recompensa de 1, el agente debe tener la suerte suficiente para encontrar la secuencia correcta de acciones. La tabla Q permanecer√° llena de ceros hasta que el agente alcance aleatoriamente la meta."]},{"cell_type":"markdown","metadata":{"id":"_haMRmsiYpOk"},"source":["Sabemos que obtenemos una recompensa de 1 cuando llegamos a la meta G. El valor-Q del estado junto a G (G-1) con la acci√≥n relevante para llegar a G se actualiza gracias a la recompensa. Se termina el episodio: el agente gan√≥ y se reinicia el juego. Ahora, la pr√≥xima vez que el agente est√© en un estado junto a G-1, aumentar√° el valor-Q del estado (llam√©moslo G-2) con la acci√≥n relevante para llegar a G-1. La pr√≥xima vez que el agente est√© en un estado junto a G-2, har√° lo mismo. se reinicia y repite, hasta que la actualizaci√≥n alcance el estado inicial S."]},{"cell_type":"markdown","metadata":{"id":"WazgAou9ac-J"},"source":["Implementar el algoritmo Q-Learning para actualizar los valores-Q de la tabla-Q, recordar la ecuaci√≥n de actualizaci√≥n:\n","\n","$$\n","Q_{new}(s_t,a_t) =  Q(s_t,a_t) + \\eta\\cdot(r_t + \\gamma\\cdot\\max_a Q(s_{t+1}, a) - Q(s_t,a_t))\n","$$"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715609415881,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"KAsS6tbuacqb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def Q_learning(env, n_episodes, eta=0.5, gamma=0.9):\n","\n","    n_states = env.observation_space.n  # = 16\n","    n_actions = env.action_space.n      # = 4\n","    qtable = np.zeros((n_states, n_actions))\n","\n","    for _ in range(n_episodes):\n","        #reiniciar el agente\n","\n","        done = False\n","        #mientras no se finalice el episodio\n","        while not done:\n","            #selecciono una acci√≥n a partir del qtable para el estado actual\n","            #si son todos ceros, selecciono una acci√≥n al azar\n","            if np.max(qtable[state])>0:\n","                action=np.argmax(qtable[state])\n","            else:\n","                action=env.action_space.sample()\n","            #implemento la acci√≥n usando step\n","            new_state, reward, done, info, p = environment.step(action)\n","\n","            #actualizo los valores de qtable\n","            qtable[state, action] = qtable[state, action]+eta*(reward+gamma*np.max(qtable[new_state]))\n","\n","            #actualizo el estado\n","            state = new_state\n","\n","    return qtable"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"f4Gvuh1RZnG0"},"outputs":[{"ename":"UnboundLocalError","evalue":"cannot access local variable 'state' where it is not associated with a value","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m environment \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m Qt \u001b[38;5;241m=\u001b[39m \u001b[43mQ_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m Qt\n","Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mQ_learning\u001b[1;34m(env, n_episodes, eta, gamma)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#mientras no se finalice el episodio\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#selecciono una acci√≥n a partir del qtable para el estado actual\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#si son todos ceros, selecciono una acci√≥n al azar\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(qtable[\u001b[43mstate\u001b[49m])\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m         action\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(qtable[state])\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'state' where it is not associated with a value"]}],"source":["environment = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","\n","Qt = Q_learning(environment, n_episodes=1000, eta=0.5, gamma=0.9)\n","Qt"]},{"cell_type":"markdown","metadata":{"id":"LTb2EL4YTBTW"},"source":["A continuaci√≥n implemente la pol√≠tica que retorne la acci√≥n del agente usando la Tabla-Q aprendida"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":246,"status":"ok","timestamp":1715609435901,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"nsIXMnsv_ROy"},"outputs":[],"source":["def basic_policy(env, Q_table, state):\n","    \"\"\"Pol√≠tica del agente\"\"\"\n","    if np.max(Q_table[state]) > 0:\n","        return np.argmax(Q_table[state])\n","    else:\n","        return env.action_space.sample()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1715609431791,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"GGYXMSaL-Ta_"},"outputs":[],"source":["# Funciones para animaci√≥n\n","def animate(num, patch, frames):\n","    patch.set_data(frames[num])\n","    return patch,\n","\n","def plot_animation(frames, repeat=False, interval=40):\n","    fig = plt.figure()\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    plt.close()\n","    anim = FuncAnimation(\n","        fig, animate, fargs=(patch, frames),\n","        frames=len(frames), repeat=repeat, interval=interval)\n","    return anim\n","\n","def show_one_episode(policy, Q_table):\n","    sequence = []\n","    frames = []\n","    env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","    state, info = env.reset()\n","    frames.append(env.render())\n","    done = False\n","    while not done:\n","        action = policy(env, Q_table, state)\n","        sequence.append(action)\n","        new_state, reward, done, info, p = env.step(action)\n","        frames.append(env.render())\n","        state = new_state\n","    print(frames[0].shape)\n","    return plot_animation(frames)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"IvY0SSM9GSRV"},"outputs":[{"ename":"NameError","evalue":"name 'Qt' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m animation, rc\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[1;32m----> 4\u001b[0m anim \u001b[38;5;241m=\u001b[39m show_one_episode(policy\u001b[38;5;241m=\u001b[39mbasic_policy, Q_table\u001b[38;5;241m=\u001b[39m\u001b[43mQt\u001b[49m)\n\u001b[0;32m      5\u001b[0m rc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimation\u001b[39m\u001b[38;5;124m'\u001b[39m, html\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjshtml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m anim\n","\u001b[1;31mNameError\u001b[0m: name 'Qt' is not defined"]}],"source":["from matplotlib import animation, rc\n","from IPython.display import HTML\n","\n","anim = show_one_episode(policy=basic_policy, Q_table=Qt)\n","rc('animation', html='jshtml')\n","anim"]},{"cell_type":"markdown","metadata":{"id":"D0koxWD5SbPk"},"source":["Hay algo a tener en cuenta con el enfoque anterior: el agente siempre elige la acci√≥n con el valor m√°s alto. Entonces, cada vez que un par estado-acci√≥n comienza a tener un valor distinto de cero, el agente siempre lo elegir√°. Las otras acciones nunca se tomar√°n, lo que significa que nunca actualizaremos su valor ¬øqu√© pasa si una de estas acciones es mejor que la que el agente siempre elige? ¬øNo deber√≠amos animar al agente a probar cosas nuevas de vez en cuando y ver si puede mejorar?"]},{"cell_type":"markdown","metadata":{"id":"TmF466c0Ydk9"},"source":["Entonces, queremos permitir que nuestro agente:\n","\n","1. Tome la acci√≥n con el valor m√°s alto (explotaci√≥n).\n","2. Elija una acci√≥n al azar para intentar encontrar incluso mejores acciones (exploraci√≥n).\n","\n","Es importante encontrar un equilibrio entre estos dos comportamientos: si el agente se enfoca solo en la explotaci√≥n, no puede probar nuevas soluciones y, por lo tanto, deja de aprender. Por otro lado, si el agente solo toma acciones al azar, el entrenamiento es in√∫til ya que no utiliza la tabla Q.\n","\n","Entonces, queremos cambiar este par√°metro con el tiempo: al principio del entrenamiento, queremos explorar el entorno tanto como sea posible. Pero la exploraci√≥n se vuelve menos interesante a medida que el agente ya conoce todos los pares estado-acci√≥n posibles. Este par√°metro representa la cantidad de aleatoriedad en la selecci√≥n de acciones.\n","\n","Esta t√©cnica se conoce com√∫nmente como el algoritmo epsilon-greedy, donde epsilon es nuestro par√°metro. Es un m√©todo simple pero extremadamente eficiente para encontrar un buen equilibrio. Cada vez que el agente tiene que tomar una acci√≥n, tiene una probabilidad Œµ de elegir una al azar, y una probabilidad 1-Œµ de elegir la que tiene el valor m√°s alto. Podemos disminuir el valor de epsilon al final de cada episodio en una cantidad fija (decaimiento lineal)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"civQMGfhNbIM"},"outputs":[],"source":["def epsilon_greedy(env, n_episodes, eta=0.5, gamma=0.9, epsilon=1, decay=0.001):\n","\n","    n_states = env.observation_space.n  # = 16\n","    n_actions = env.action_space.n      # = 4\n","    qtable = np.zeros((n_states, n_actions))\n","\n","    for _ in range(n_episodes):\n","        #reiniciar el agente\n","        state, _ = env.reset()\n","        done = False\n","        #mientras no se finalice el episodio\n","        while not done:\n","            # genero un n√∫mero aleatorio entre 0 y 1\n","\n","\n","            # si el n√∫mero es menor a epsilon\n","            # tomo una acci√≥n aleatorio\n","            # sino uso la qtable\n","\n","\n","\n","            #implemento la acci√≥n usando step\n","\n","\n","            #actualizo los valores de qtable\n","\n","\n","            #actualizo el estado\n","            state = new_state\n","\n","        #actualizo epsilon restando el decaimiento\n","\n","\n","    return qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-juLIDoUTzzv"},"outputs":[],"source":["env2 = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","\n","Qt = epsilon_greedy(env2, n_episodes=10000, eta=0.2, gamma=0.9)\n","Qt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_m24ghN3aAtf"},"outputs":[],"source":["from matplotlib import animation, rc\n","from IPython.display import HTML\n","\n","anim = show_one_episode(policy=basic_policy, Q_table=Qt)\n","rc('animation', html='jshtml')\n","anim"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyM2zqpkr+SLVGy7i9Gg6cEh","gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
