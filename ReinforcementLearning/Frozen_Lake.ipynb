{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28774,"status":"ok","timestamp":1715609385696,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"VGfORq6ZN9cQ","outputId":"bc63f59d-53c5-4a74-ae0c-dd38ae31a663"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gymnasium in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (3.0.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (4.11.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\anaso\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (0.0.4)\n"]}],"source":["!pip install gymnasium"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1153,"status":"ok","timestamp":1715609386846,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"eQnGRUcqOK3F"},"outputs":[],"source":["import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from matplotlib.animation import FuncAnimation"]},{"cell_type":"markdown","metadata":{"id":"5PtJFZsxObHQ"},"source":["Frozen Lake es un entorno simple compuesto por casillas, donde el agente debe moverse desde una casilla inicial hasta una meta.\n","\n","Las casillas pueden ser un lago congelado seguro ✅ o un agujero ❌ que te deja atrapado por siempre.\n","\n","El agente, tiene 4 acciones posibles: ir ◀️IZQUIERDA, 🔽ABAJO, ▶️DERECHA o 🔼ARRIBA.\n","\n","El agente debe aprender a evitar los agujeros para poder alcanzar la meta en el menor número de acciones posible.\n","\n","Por defecto, el entorno siempre tiene la misma configuración. En el código del entorno, cada casilla está representada por una letra de la siguiente manera:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6Q4YK5CTPGZi"},"outputs":[],"source":["# S F F F       (S: inicio, seguro)\n","# F H F H       (F: superficie congelada, seguro)\n","# F F F H       (H: agujero, atrapado por siempre)\n","# H F F G       (G: meta, a salvo)"]},{"cell_type":"markdown","metadata":{"id":"dRlV0hD0O1Kr"},"source":["De hecho, es muy fácil encontrar varias soluciones correctas: DERECHA → DERECHA → ABAJO → ABAJO → ABAJO → DERECHA es una obvia.\n","\n","Pero podríamos hacer una secuencia de acciones que rodee un agujero 10 veces antes de llegar a la meta. Esta secuencia es válida, pero no cumple con nuestro requisito final: el agente necesita alcanzar la meta en el menor número de acciones posible.\n","\n","En este ejemplo, el número mínimo de acciones para completar el juego es 6. Necesitamos recordar este hecho para comprobar si nuestro agente realmente domina Frozen Lake o no."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1715609401310,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"5loEIfuTOOAc","outputId":"68fb8e6f-627c-46fa-8027-7b94bb6f7bac"},"outputs":[{"ename":"DependencyNotInstalled","evalue":"pygame is not installed, run `pip install gymnasium[toy-text]`","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:342\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m environment \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m environment\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 3\u001b[0m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:65\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:362\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[0;32m    358\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         )\n\u001b[1;32m--> 362\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m     _check_render_return(env\u001b[38;5;241m.\u001b[39mrender_mode, result)\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:344\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install gymnasium[toy-text]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n","\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gymnasium[toy-text]`"]}],"source":["environment = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","environment.reset()\n","environment.render()"]},{"cell_type":"markdown","metadata":{"id":"jDHzy5q3REkK"},"source":["En Frozen Lake, hay 16 casillas, lo que significa que nuestro agente puede encontrarse en 16 posiciones diferentes o estados.\n","\n","Para cada estado, hay 4 acciones posibles: ir ◀️IZQUIERDA, 🔽ABAJO, ▶️DERECHA, y 🔼ARRIBA.\n","\n","El agente debe aprender qué acción elegir en cada estado. Para saber qué acción es la mejor en un estado dado, vamos a calcular el Valor-Q de cada acción (Recordar que este valor es función del estado y la acción).\n","\n","Tenemos 16 estados y 4 acciones, por lo que queremos calcular 16 x 4 = 64 valores Q."]},{"cell_type":"markdown","metadata":{"id":"ODx4VoV2RW-a"},"source":["Una forma práctica de representarlo es mediante una tabla, conocida como tabla Q, donde las filas enumeran cada estado s y las columnas enumeran cada acción a.\n","\n","En esta tabla Q, cada celda contiene el valor $Q(s, a)$, que son las recompensas esperadas de tomar la acción $a$ en el estado $s$\n","\n","Cuando nuestro agente se encuentra en un estado particular $s$, solo tiene que consultar esta tabla para ver qué acción tiene el valor más alto."]},{"cell_type":"markdown","metadata":{"id":"I8e82ltMScc_"},"source":["``\n","S     ◀️LEFT    🔽DOWN    ▶️RIGHT   🔼UP \\\n","0     Q(0,◀️)   Q(0,🔽)   Q(0,▶️)   Q(0,🔼) \\\n","1     Q(1,◀️)   Q(1,🔽)   Q(1,▶️)   Q(1,🔼) \\\n","2     Q(2,◀️)   Q(2,🔽)   Q(2,▶️)   Q(2,🔼) \\\n",". \\\n","14    Q(14,◀️)  Q(14,🔽)  Q(14,▶️)  Q(14,🔼) \\\n","G     Q(15,◀️)  Q(15,🔽)  Q(15,▶️)  Q(15,🔼)\n","``\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zFrrdi1dRVNH"},"outputs":[{"name":"stdout","output_type":"stream","text":["Q-table =\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["# Creamos la tabla y la inicializamos con ceros\n","# dimensión (filas x columnas) = (s x a) = 16 x 4\n","\n","nb_states = environment.observation_space.n  # = 16\n","nb_actions = environment.action_space.n      # = 4\n","qtable = np.zeros((nb_states, nb_actions))\n","\n","print('Q-table =')\n","print(qtable)"]},{"cell_type":"markdown","metadata":{"id":"UdTqKkgST7e-"},"source":["``\n","◀️ LEFT = 0 \\\n","🔽 DOWN = 1 \\\n","▶️ RIGHT = 2 \\\n","🔼 UP = 3\n","``"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"71tjf_CET5k7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(0, {'prob': 1})\n","1\n","recompensa: 0.0\n","Done: False\n"]},{"ename":"DependencyNotInstalled","evalue":"pygame is not installed, run `pip install gymnasium[toy-text]`","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:342\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecompensa:\u001b[39m\u001b[38;5;124m'\u001b[39m, reward)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone:\u001b[39m\u001b[38;5;124m'\u001b[39m, done)\n\u001b[1;32m---> 13\u001b[0m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\anaso\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:344\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install gymnasium[toy-text]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n","\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gymnasium[toy-text]`"]}],"source":["# Para tomar una acción al azar\n","print(environment.reset())\n","\n","accion = environment.action_space.sample()\n","new_state, reward, done, info, p = environment.step(accion) # Retorna done=True si el agente cae a un agujero o llega a la meta\n","\n","accion2 = environment.action_space.sample()\n","new_state, reward, done, info, p = environment.step(accion2) # Retorna done=True si el agente cae a un agujero o llega a la meta\n","\n","print(accion)\n","print(f'recompensa:', reward)\n","print(f'Done:', done)\n","environment.render()"]},{"cell_type":"markdown","metadata":{"id":"U7r0PUxZWTzu"},"source":["La recompensa es 0 y solo un estado puede darnos una recompensa positiva en todo el juego. Si queremos ver una recompensa de 1, el agente debe tener la suerte suficiente para encontrar la secuencia correcta de acciones. La tabla Q permanecerá llena de ceros hasta que el agente alcance aleatoriamente la meta."]},{"cell_type":"markdown","metadata":{"id":"_haMRmsiYpOk"},"source":["Sabemos que obtenemos una recompensa de 1 cuando llegamos a la meta G. El valor-Q del estado junto a G (G-1) con la acción relevante para llegar a G se actualiza gracias a la recompensa. Se termina el episodio: el agente ganó y se reinicia el juego. Ahora, la próxima vez que el agente esté en un estado junto a G-1, aumentará el valor-Q del estado (llamémoslo G-2) con la acción relevante para llegar a G-1. La próxima vez que el agente esté en un estado junto a G-2, hará lo mismo. se reinicia y repite, hasta que la actualización alcance el estado inicial S."]},{"cell_type":"markdown","metadata":{"id":"WazgAou9ac-J"},"source":["Implementar el algoritmo Q-Learning para actualizar los valores-Q de la tabla-Q, recordar la ecuación de actualización:\n","\n","$$\n","Q_{new}(s_t,a_t) =  Q(s_t,a_t) + \\eta\\cdot(r_t + \\gamma\\cdot\\max_a Q(s_{t+1}, a) - Q(s_t,a_t))\n","$$"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715609415881,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"KAsS6tbuacqb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def Q_learning(env, n_episodes, eta=0.5, gamma=0.9):\n","\n","    n_states = env.observation_space.n  # = 16\n","    n_actions = env.action_space.n      # = 4\n","    qtable = np.zeros((n_states, n_actions))\n","\n","    for _ in range(n_episodes):\n","        #reiniciar el agente\n","\n","        done = False\n","        #mientras no se finalice el episodio\n","        while not done:\n","            #selecciono una acción a partir del qtable para el estado actual\n","            #si son todos ceros, selecciono una acción al azar\n","            if np.max(qtable[state])>0:\n","                action=np.argmax(qtable[state])\n","            else:\n","                action=env.action_space.sample()\n","            #implemento la acción usando step\n","            new_state, reward, done, info, p = environment.step(action)\n","\n","            #actualizo los valores de qtable\n","            qtable[state, action] = qtable[state, action]+eta*(reward+gamma*np.max(qtable[new_state]))\n","\n","            #actualizo el estado\n","            state = new_state\n","\n","    return qtable"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"f4Gvuh1RZnG0"},"outputs":[{"ename":"UnboundLocalError","evalue":"cannot access local variable 'state' where it is not associated with a value","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m environment \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m Qt \u001b[38;5;241m=\u001b[39m \u001b[43mQ_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m Qt\n","Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mQ_learning\u001b[1;34m(env, n_episodes, eta, gamma)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#mientras no se finalice el episodio\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#selecciono una acción a partir del qtable para el estado actual\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m#si son todos ceros, selecciono una acción al azar\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(qtable[\u001b[43mstate\u001b[49m])\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m         action\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(qtable[state])\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'state' where it is not associated with a value"]}],"source":["environment = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","\n","Qt = Q_learning(environment, n_episodes=1000, eta=0.5, gamma=0.9)\n","Qt"]},{"cell_type":"markdown","metadata":{"id":"LTb2EL4YTBTW"},"source":["A continuación implemente la política que retorne la acción del agente usando la Tabla-Q aprendida"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":246,"status":"ok","timestamp":1715609435901,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"nsIXMnsv_ROy"},"outputs":[],"source":["def basic_policy(env, Q_table, state):\n","    \"\"\"Política del agente\"\"\"\n","    if np.max(Q_table[state]) > 0:\n","        return np.argmax(Q_table[state])\n","    else:\n","        return env.action_space.sample()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1715609431791,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"},"user_tz":180},"id":"GGYXMSaL-Ta_"},"outputs":[],"source":["# Funciones para animación\n","def animate(num, patch, frames):\n","    patch.set_data(frames[num])\n","    return patch,\n","\n","def plot_animation(frames, repeat=False, interval=40):\n","    fig = plt.figure()\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    plt.close()\n","    anim = FuncAnimation(\n","        fig, animate, fargs=(patch, frames),\n","        frames=len(frames), repeat=repeat, interval=interval)\n","    return anim\n","\n","def show_one_episode(policy, Q_table):\n","    sequence = []\n","    frames = []\n","    env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","    state, info = env.reset()\n","    frames.append(env.render())\n","    done = False\n","    while not done:\n","        action = policy(env, Q_table, state)\n","        sequence.append(action)\n","        new_state, reward, done, info, p = env.step(action)\n","        frames.append(env.render())\n","        state = new_state\n","    print(frames[0].shape)\n","    return plot_animation(frames)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"IvY0SSM9GSRV"},"outputs":[{"ename":"NameError","evalue":"name 'Qt' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m animation, rc\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[1;32m----> 4\u001b[0m anim \u001b[38;5;241m=\u001b[39m show_one_episode(policy\u001b[38;5;241m=\u001b[39mbasic_policy, Q_table\u001b[38;5;241m=\u001b[39m\u001b[43mQt\u001b[49m)\n\u001b[0;32m      5\u001b[0m rc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimation\u001b[39m\u001b[38;5;124m'\u001b[39m, html\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjshtml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m anim\n","\u001b[1;31mNameError\u001b[0m: name 'Qt' is not defined"]}],"source":["from matplotlib import animation, rc\n","from IPython.display import HTML\n","\n","anim = show_one_episode(policy=basic_policy, Q_table=Qt)\n","rc('animation', html='jshtml')\n","anim"]},{"cell_type":"markdown","metadata":{"id":"D0koxWD5SbPk"},"source":["Hay algo a tener en cuenta con el enfoque anterior: el agente siempre elige la acción con el valor más alto. Entonces, cada vez que un par estado-acción comienza a tener un valor distinto de cero, el agente siempre lo elegirá. Las otras acciones nunca se tomarán, lo que significa que nunca actualizaremos su valor ¿qué pasa si una de estas acciones es mejor que la que el agente siempre elige? ¿No deberíamos animar al agente a probar cosas nuevas de vez en cuando y ver si puede mejorar?"]},{"cell_type":"markdown","metadata":{"id":"TmF466c0Ydk9"},"source":["Entonces, queremos permitir que nuestro agente:\n","\n","1. Tome la acción con el valor más alto (explotación).\n","2. Elija una acción al azar para intentar encontrar incluso mejores acciones (exploración).\n","\n","Es importante encontrar un equilibrio entre estos dos comportamientos: si el agente se enfoca solo en la explotación, no puede probar nuevas soluciones y, por lo tanto, deja de aprender. Por otro lado, si el agente solo toma acciones al azar, el entrenamiento es inútil ya que no utiliza la tabla Q.\n","\n","Entonces, queremos cambiar este parámetro con el tiempo: al principio del entrenamiento, queremos explorar el entorno tanto como sea posible. Pero la exploración se vuelve menos interesante a medida que el agente ya conoce todos los pares estado-acción posibles. Este parámetro representa la cantidad de aleatoriedad en la selección de acciones.\n","\n","Esta técnica se conoce comúnmente como el algoritmo epsilon-greedy, donde epsilon es nuestro parámetro. Es un método simple pero extremadamente eficiente para encontrar un buen equilibrio. Cada vez que el agente tiene que tomar una acción, tiene una probabilidad ε de elegir una al azar, y una probabilidad 1-ε de elegir la que tiene el valor más alto. Podemos disminuir el valor de epsilon al final de cada episodio en una cantidad fija (decaimiento lineal)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"civQMGfhNbIM"},"outputs":[],"source":["def epsilon_greedy(env, n_episodes, eta=0.5, gamma=0.9, epsilon=1, decay=0.001):\n","\n","    n_states = env.observation_space.n  # = 16\n","    n_actions = env.action_space.n      # = 4\n","    qtable = np.zeros((n_states, n_actions))\n","\n","    for _ in range(n_episodes):\n","        #reiniciar el agente\n","        state, _ = env.reset()\n","        done = False\n","        #mientras no se finalice el episodio\n","        while not done:\n","            # genero un número aleatorio entre 0 y 1\n","\n","\n","            # si el número es menor a epsilon\n","            # tomo una acción aleatorio\n","            # sino uso la qtable\n","\n","\n","\n","            #implemento la acción usando step\n","\n","\n","            #actualizo los valores de qtable\n","\n","\n","            #actualizo el estado\n","            state = new_state\n","\n","        #actualizo epsilon restando el decaimiento\n","\n","\n","    return qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-juLIDoUTzzv"},"outputs":[],"source":["env2 = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n","\n","Qt = epsilon_greedy(env2, n_episodes=10000, eta=0.2, gamma=0.9)\n","Qt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_m24ghN3aAtf"},"outputs":[],"source":["from matplotlib import animation, rc\n","from IPython.display import HTML\n","\n","anim = show_one_episode(policy=basic_policy, Q_table=Qt)\n","rc('animation', html='jshtml')\n","anim"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyM2zqpkr+SLVGy7i9Gg6cEh","gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
